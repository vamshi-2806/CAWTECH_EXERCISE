
# Local Document Q&A System using RAG + Ollama

A fully local, privacy-first intelligent document assistant that answers questions using uploaded documents.  
It combines **RAG (Retrieval-Augmented Generation)** with **Ollama-powered LLMs** like `phi3`, `llama3`, or `mistral`.

---

## üöÄ Features

- ‚úÖ Upload PDF, DOCX, or TXT documents
- üß† Chunk, embed and index content locally with `MiniLM` + `ChromaDB`
- üîé Query using FastAPI ‚Äî retrieves relevant context using RAG
- ü§ñ Answer generation via **local Ollama models**
- üìù Feedback collection endpoint
- ‚ö° 100% local ‚Äî no API keys, no internet required for inference

---

## üõ†Ô∏è Technologies Used

- [FastAPI](https://fastapi.tiangolo.com/)
- [ChromaDB](https://www.trychroma.com/) for vector storage
- [sentence-transformers](https://www.sbert.net/) for embeddings
- [Ollama](https://ollama.com/) to run LLMs locally
- [PyMuPDF](https://pymupdf.readthedocs.io/en/latest/) and `docx2txt` for document parsing

---

## üì¶ Folder Structure

```

üìÅ app
‚î£ üìÇ routers         # API endpoints
‚î£ üìÇ utils           # Embedding, parsing, chunking
‚î£ üìÇ vectorstore     # ChromaDB integration
‚îó main.py            # FastAPI app entrypoint
üìÅ uploads             # Stores uploaded documents
.env                  # Ollama model key (optional)
requirements.txt

````

---

## üß™ Quickstart

### 1. Clone the repo
```bash
git clone https://github.com/vamshi-2806/CAWTECH_EXERCISE.git
cd CAWTECH_EXERCISE
````

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Start Ollama (first time pulls the model)

```bash
ollama pull phi3
ollama run phi3
```

> üí° You can swap `phi3` with `llama3`, `mistral`, or any Ollama-supported model.

### 4. Start the backend

```bash
uvicorn app.main:app --reload
```

---

## üåê API Endpoints (via Swagger UI)

Open browser:

```
http://127.0.0.1:8000/docs
```

### Available Routes:

| Method | Route                   | Description                    |
| ------ | ----------------------- | ------------------------------ |
| GET    | `/`                     | Health check / Welcome message |
| POST   | `/document/upload/`     | Upload a document              |
| GET    | `/qa/ask/?question=...` | Ask a question using local LLM |
| POST   | `/feedback/`            | Submit feedback                |

---

## üì§ Example Usage

1. Upload a document using `/document/upload/`
2. Ask:

```
/qa/ask/?question=What is semantic chunking?
```

3. Get a response like:

```json
{
  "question": "What is semantic chunking?",
  "retrieved_chunks": [...],
  "response": "Semantic chunking is the process of..."
}
```

---

## üß† Future Ideas

* Add Streamlit or React frontend
* Persist feedback to file or DB
* Support multi-doc queries
* Add support for Claude, Gemini, or OpenAI (optional)

---

## üôå Credits

* Built using [Ollama](https://ollama.com), [FastAPI](https://fastapi.tiangolo.com/), and [ChromaDB](https://www.trychroma.com/)
* Powered by local language models for 100% private document Q\&A

---

